{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已添加到路径的目录： /wuzhou/pentafleet/b2312_/HRM-main\n",
      "当前Python路径包含： True\n",
      "inputs shape: (7290, 64) labels shape: (7290, 64)\n",
      "first seq tokens: [33 38 30 20 30 37 35 25 40 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "first labels tokens: [-100 -100 -100 -100 -100 -100 -100 -100   40   32 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100]\n",
      "decoded input: 2 7   +   6 4 = 9 1\n",
      "first supervised pos: [8 9] decoded target tokens: ['9', '1']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 手动指定需要添加的路径（根据实际目录层级调整）\n",
    "# 假设当前在 HRM-main/notebooks，上级目录是 HRM-main，用 \"..\" 表示\n",
    "target_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(target_dir)  # 将 HRM-main 目录添加到Python路径\n",
    "\n",
    "# 验证是否添加成功（可选）\n",
    "print(\"已添加到路径的目录：\", target_dir)\n",
    "print(\"当前Python路径包含：\", target_dir in sys.path)  # 应输出 True\n",
    "\n",
    "import numpy as np\n",
    "from mytokenizers.hf_math_tokenizer import HFMathTokenizer\n",
    "\n",
    "tokenizer = HFMathTokenizer()\n",
    "d = np.load(\"../data/MATH-401/exp5/train/all__inputs.npy\")\n",
    "L = np.load(\"../data/MATH-401/exp5/train/all__labels.npy\")\n",
    "print(\"inputs shape:\", d.shape, \"labels shape:\", L.shape)\n",
    "i = 0\n",
    "print(\"first seq tokens:\", d[i][:50])\n",
    "print(\"first labels tokens:\", L[i][:50])\n",
    "# decode visible tokens (stop at pad)\n",
    "def decode_seq(arr):\n",
    "    toks = [int(x) for x in arr if x != tokenizer.pad_token_id]\n",
    "    return tokenizer.decode(toks)\n",
    "print(\"decoded input:\", decode_seq(d[i]))\n",
    "# 找出第一个 label 非 IGNORE 的位置（即模型应该预测的位置）\n",
    "pos = np.where(L[i] != -100)[0]\n",
    "print(\"first supervised pos:\", pos[:10], \"decoded target tokens:\", [tokenizer.decode([int(L[i,p])]) for p in pos[:5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 41\n",
      "pad_token_id: 0\n",
      "unk_token_id: 1\n",
      "bos_token_id: 2\n",
      "eos_token_id: 3\n",
      "'0' -> [31]\n",
      "'1' -> [32]\n",
      "'2' -> [33]\n",
      "'3' -> [34]\n",
      "'4' -> [35]\n",
      "'5' -> [36]\n",
      "'6' -> [37]\n",
      "'7' -> [38]\n",
      "'8' -> [39]\n",
      "'9' -> [40]\n",
      "'+' -> [20]\n",
      "'=' -> [25]\n",
      "'.' -> [28]\n",
      "'-' -> [21]\n",
      "' ' -> [30]\n",
      "inputs shape: (7290, 64) labels shape: (7290, 64)\n",
      "first input tokens (slice): [33 38 30 20 30 37 35 25 40 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n",
      "first label tokens (slice): [-100 -100 -100 -100 -100 -100 -100 -100   40   32 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100]\n",
      "decoded input (ignoring pad): 2 7   +   6 4 = 9 1\n",
      "indices where labels != pad_token_id: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mytokenizers.hf_math_tokenizer import HFMathTokenizer\n",
    "\n",
    "tokenizer = HFMathTokenizer()\n",
    "print(\"vocab_size:\", tokenizer.vocab_size)\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "print(\"unk_token_id:\", getattr(tokenizer, \"unk_token_id\", None))\n",
    "print(\"bos_token_id:\", getattr(tokenizer, \"bos_token_id\", None))\n",
    "print(\"eos_token_id:\", getattr(tokenizer, \"eos_token_id\", None))\n",
    "\n",
    "# 显示几个字符的 id（0-9, +, =, .）\n",
    "for ch in list(\"0123456789+=.- \"):\n",
    "    ids = tokenizer.encode(ch, add_special_tokens=False)\n",
    "    print(repr(ch), \"->\", ids)\n",
    "\n",
    "inputs = np.load(\"../data/MATH-401/exp5/train/all__inputs.npy\")\n",
    "labels = np.load(\"../data/MATH-401/exp5/train/all__labels.npy\")\n",
    "i = 0\n",
    "row_in = inputs[i]\n",
    "row_lab = labels[i]\n",
    "print(\"inputs shape:\", inputs.shape, \"labels shape:\", labels.shape)\n",
    "print(\"first input tokens (slice):\", row_in[:30])\n",
    "print(\"first label tokens (slice):\", row_lab[:30])\n",
    "print(\"decoded input (ignoring pad):\", tokenizer.decode([int(x) for x in row_in if int(x) != tokenizer.pad_token_id]))\n",
    "print(\"indices where labels != pad_token_id:\", np.where(row_lab != tokenizer.pad_token_id)[0][:50])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrmpy311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
